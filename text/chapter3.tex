\chapter{関連研究}
\label{exprmnt:physarum}

\section{AR アプリケーションにおけるコンテンツの動的更新手法}

博物館や美術館における AR 展示システムの運用において、展示内容の変更や追加に合わせてアプリケーションを柔軟に更新できることは極めて重要である。 Ohlei ら [1] は、博物館の専門家がプログラミングの知識なしに AR ツアーを作成および編集できるシステムとして、 Ambient Learning Spaces (ALS) フレームワークの一部である InfoGrid を提案している。そのシステム構成を図\ref{fig:infogrid_arch}に示す。

図

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=70mm]{./figs/関連研究/1ホットアップデート/infogrid_arch.png}
\caption[InfoGrid システムの構成]{InfoGrid システムの構成 [1].}
\label{fig:infogrid_arch}
\end{center}
\end{figure}

このシステムは、 Web ベースのポータルサイト (ALS Portal) とモバイル AR アプリ、そしてバックエンドシステム (NEMO) から構成される。彼らは、静的な画像や動画だけでなく、アニメーションやインタラクティブな挙動を含む 3D オブジェクトを動的に扱うために、 Asset Collections と呼ばれる新しいオーバーレイタイプを開発した。これは Unity AssetBundle 機能を活用したもので、 Unity エディタ用のアドオンツールを用いて作成されたモデルやテクスチャ、アニメーションなどのアセット群を、ランタイムでモバイルアプリにロードする仕組みである。 InfoGrid アプリは、起動時に NEMO バックエンドからツアー情報を取得し、必要な Asset Collections を動的にダウンロードする。これにより、アプリ自体をアプリストア経由で更新することなく、展示物に対して動的な 3D コンテンツを追加したり、アプリ内の管理者機能で配置（移動や回転、拡大縮小）を調整したりすることが可能となる。彼らは自然史博物館における実証実験を通じて、このシステムが System Usability Scale (SUS) 86.04 という高いスコアを記録し、専門的な開発スキルを持たない博物館スタッフでも高度な AR ツアーの運用が可能であることを示している。

また、 AR 展示制作のアクセシビリティ向上とコンテンツ配信の効率化に関して、 Duanmu ら [2] は MUSE システムを設計および実装している。彼らの研究は、 3D アーティストやキュレーターが、複雑な技術的障壁なしに自身の作品を AR 展示できることを目的としている。 MUSE システムは、コンテンツ制作側の Unity Toolkit と、鑑賞者側の Viewer App からなる分離型アーキテクチャを採用しており、そのワークフローは図\ref{fig:muse_workflow}の通りである。

図

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=70mm]{./figs/関連研究/1ホットアップデート/muse_workflow.png}
\caption[MUSE システムのワークフロー]{MUSE システムのワークフロー [2].}
\label{fig:muse_workflow}
\end{center}
\end{figure}

Unity Toolkit は Unity エディタ上で動作し、 3D モデルのインポートからカスタマイズ、そして AssetBundle 形式へのパッキングまでを一貫して支援する。特に Config Generator 機能により、展示物のタイトルや説明、ファイル名などのメタデータを記述した設定ファイル (XML) を生成し、これをクラウドサーバー (Object Storage Service) にアップロードすることで展示構成を管理する。一方、 Viewer App は Unity の AR Foundation を用いて開発されており、展示会場の QR コードをスキャンすることで、対応する設定ファイルを読み込み、サーバーから必要な 3D モデル (AssetBundle) をオンデマンドでダウンロードして表示する。 QR コードは空間的なアンカーとしてだけでなく、コンテンツへのディープリンクとしても機能する。このアーキテクチャにより、設定ファイルを書き換えるだけで展示リストの更新やコンテンツの差し替えを行うホットフィックスが可能となり、鑑賞者用アプリの再配布を不要にしている。

これらの研究は、 Unity の AssetBundle 機構とクラウド通信を組み合わせることで、アプリケーションのバイナリ更新を回避し、アセットレベルでの動的更新（ホットアップデート）を実現した成功例であると言える。次節では、こうしたコンテンツ更新の仕組みに加え、位置情報技術と統合することで体験を共有可能にするシステムについて述べる。

\section{クラウド統合型位置情報ベース AR コンテンツ共有システム}

前節で述べたコンテンツの動的管理に加え、近年では AR (Augmented Reality) や MR (Mixed Reality) 技術において、クラウドコンピューティングや高度な位置特定技術を統合することで、単一のデバイス内で完結していた体験を、複数のユーザー間で共有かつ持続させる試みが活発に行われている。特に、特定の場所に紐づいたコンテンツをクラウド経由で管理および配信するシステムは、文化遺産の保護、ユーザー生成コンテンツ (UGC) の普及、そして芸術鑑賞の深化といった多様な領域で応用が進んでいる。

Bekele [3] は 2021 年、バーチャルヘリテージ (VH) 分野において、クラウドコンピューティングと MR 技術を統合した Clouds-Based Collaborative and Multi-Modal MR システムを提案した。従来の MR アプリケーションはデバイスの計算リソースやストレージ容量に依存するため、高精細な 3D モデルやマルチメディアコンテンツの扱いに制約があり、またアプリケーション自体の長期的な保存や維持も課題であった。そこで Bekele らは、 Amazon Web Services (AWS) と Microsoft Azure という異なるクラウドプロバイダーのサービスを複合的に活用するアーキテクチャを構築した（図 \ref{fig:bekele_arch}）。具体的には、 Azure Spatial Anchors を用いて複数の HoloLens デバイス間で空間アンカー (Spatial Anchors) の識別子を共有し、 Azure Cosmos DB および Azure App Service を介して同期することで、同一の現実空間におけるユーザー間の協調的なインタラクションを実現している。さらに、 Amazon Polly による音声合成機能や Amazon S3 へのデータオフロードを実装することで、デバイスの負荷を軽減しつつ、文化財に関する多言語ガイドやリッチな視聴覚体験を提供可能とした。このアプローチにより、没入型技術を用いた文化学習における社会的プレゼンス (Social Presence) と仮想的プレゼンス (Virtual Presence) の両立が示された。

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=120mm]{./figs/関連研究/2サーバ配信システム/bekele_system_arch.png}
\caption[Clouds-Based Collaborative and Multi-Modal MR のシステムアーキテクチャ]{Clouds-Based Collaborative and Multi-Modal MR のシステムアーキテクチャ [3].}
\label{fig:bekele_arch}
\end{center}
\end{figure}

続いて 2023 年、 Kidman [4] は、一般ユーザーが位置情報に基づくマーカーレス AR コンテンツをその場で制作 (In-Situ Creation) し、共有するためのプラットフォーム ARtverse を開発した。既存の AR オーサリングツールは、 QR コード等の物理マーカーに依存するか、あるいは Adobe Aero のように位置情報に紐づかない (Location-independent) コンテンツ生成に限られる傾向があり、現実世界の特定の場所に永続的なコンテンツを配置および共有する手段は限られていた。Kidman は、 Unity と ARKit 、そしてバックエンドに Google Firebase を採用し、クライアント・サーバー型のアプローチでこの課題解決を試みた。提案システムでは、ユーザーが配置した 3D アセットの情報だけでなく、その環境の空間特徴点マップ (ARWorldMap) をシリアライズしてクラウドデータベースに保存する（図 \ref{fig:kidman_system}）。これにより、別のユーザーが同一地点を訪れた際、保存された空間マップを読み込むことで、マーカーレスでありながら正確なコンテンツの復元 (Relocalization) を可能にしている。また、 GPS の精度誤差による位置合わせの困難さを軽減するため、コンテンツ作成時のカメラ映像（スクリーンショット）をナビゲーションの手がかりとして提示するインターフェースを導入し、ユーザー生成型 AR 体験のアクセシビリティと再現性を向上させた。

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=120mm]{./figs/関連研究/2サーバ配信システム/kidman_loading_system.png}
\caption[ARtverse におけるサーバーからのコンテンツ読み込みと自己位置推定のプロセス]{ARtverse におけるサーバーからのコンテンツ読み込みと自己位置推定のプロセス [4].}
\label{fig:kidman_system}
\end{center}
\end{figure}

さらに近年、飛田ら [5] は、美術館における対話型鑑賞を支援する目的で、 VPS (Visual Positioning System) を活用したマーカーレス AR アプリケーションを提案している。従来の対話型鑑賞は、ファシリテータを中心としたワークショップ形式で行われることが多く、時間的および空間的な制約により参加のハードルが高かった。彼らは、 E コマースにおける商品レビューやレコメンデーションの概念を実空間の美術作品に適用し、鑑賞体験の非同期的な共有モデルを構築した。本システムでは、鑑賞者が作品に対して宝石を模した AR オブジェクトを配置し、感想コメントを紐付けることができる。これらはクラウド上で管理され、他の鑑賞者は探索モードを通じて他者の残したコメントを閲覧し、多様な視点からの鑑賞を楽しむことが可能となる。技術的には、物理的なマーカーを設置せず、 VPS 技術によって高精度な位置認識を行うことで、作品や展示空間の美観を損なうことなく情報の重畳を実現している（図 \ref{fig:tobita_config}）。実証実験においては、自分の感想を即時的に記録および共有できる点や、他者のコメントを通じて新たな気づきが得られる点が評価されており、 ICT を活用した新たな鑑賞支援手法としての有効性が示唆されている。

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=100mm]{./figs/関連研究/2サーバ配信システム/tobita_system_config.png}
\caption[対話型美術鑑賞支援システムの構成図]{対話型美術鑑賞支援システムの構成図 [5].}
\label{fig:tobita_config}
\end{center}
\end{figure}

このように、クラウドと位置情報を活用したシステムは、体験の共有という側面で大きな可能性を示している。しかし、すべての来館者が視覚的な情報のみで展示を楽しめるわけではない。次節では、視覚情報に依存しないアクセシビリティ技術の統合に関する研究動向を確認する。

\section{AR 展示におけるスクリーンリーダーおよび音声読み上げ技術の活用}

博物館や美術館の展示において、視覚情報に依存しない情報伝達手段として、スクリーンリーダーや音声読み上げ技術を AR システムに統合する試みが行われている。

Guedes ら [6] は、博物館におけるアクセシビリティの向上を目的として、 AR 技術とスクリーンリーダーを統合したアプリケーション AIMuseum を開発した。本システムは、展示物に付与された QR コードを認識することで、関連するテキスト情報を取得し、 Unity の UI Accessibility Plugin を用いて多言語で読み上げる機能を有している（図 \ref{fig:aimuseum_ar}）。これにより、視覚障がい者や読字障がい者を含む多様なユーザーが、展示物の詳細情報にアクセス可能となる。 38 名の参加者を対象とした評価実験では、スクリーンリーダー機能（テキスト読み上げ）に対して 92\% の参加者が非常に満足と回答し、高い受容性が示された。特に、読字困難な参加者からは、スクリーンリーダーが作品への集中を助けるとの肯定的なフィードバックが得られており、視覚情報と音声情報の融合が展示体験の質を向上させることが確認されている。

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=70mm]{./figs/関連研究/3スクリーンリーダ/aimuseum_ar.png}
\caption[AIMuseumにおけるAR表示例]{AIMuseumにおけるAR表示例  [6].}
\label{fig:aimuseum_ar}
\end{center}
\end{figure}

一方、伏田と赤羽 [7] は、従来の視覚偏重型 AR の課題に対し、音声による情報提示に特化した Onsei AR を提案している。彼らは、スマートフォンの画面越しではなく、実空間の展示物を直接鑑賞させることを目的とし、画像マーカー認識と連動して解説やパネルの内容を音声で読み上げる機能を実装した。図 \ref{fig:onsei_ar_ui} に示すように、アプリケーションの画面には黒背景にマーカーとの距離を示す矩形のみを表示し、視覚情報を極力排除することで、利用者の視線が手元の画面ではなく実空間の作品に向くよう設計されている。

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=70mm]{./figs/関連研究/3スクリーンリーダ/onsei_ar_ui.png}
\caption[Onsei ARの画面UI]{Onsei ARの画面UI．黒背景に距離を示す矩形のみを表示し，利用者が画面を注視しないよう配慮されている [7].}
\label{fig:onsei_ar_ui}
\end{center}
\end{figure}

複数の展示会における実証実験の結果、アンケートでは動画再生機能よりも音声読み上げ機能への関心が最も高く、「画面ばかり見ない体験が面白い」や「目の見えない人向けのアプリケーションとして活用できる」といった意見が寄せられた。しかしながら、展示パネルのテキストをそのまま読み上げるだけでは、単に読む労力を軽減する以上の体験価値を提供できていないという課題も浮き彫りとなり、コンテキストに応じた音声設計の重要性が示唆されている。

これらの研究は、 AR 環境におけるスクリーンリーダーや音声読み上げ機能が、視覚障がい者へのアクセシビリティ支援だけでなく、晴眼者にとっても展示物への没入を阻害しない有効なインターフェースとなり得ることを示している。 Guedes らのシステムはアクセシビリティの観点からスクリーンリーダーを必須機能として位置づけ、伏田らのシステムは鑑賞体験の質の観点から音声読み上げの有用性を実証しており、双方は補完的な関係にあると言える。こうした情報の提示手法に加え、ユーザーがどのようにシステムを操作するかというインタラクション手法も重要な要素である。

\section{博物館展示における AR インタラクション手法とデバイス特性の比較}

博物館や展示空間における AR (Augmented Reality) の活用において、来館者が展示物といかに直感的かつ効果的にインタラクションを行うかは重要な課題である。従来の手持ち型デバイス (Handheld Device: HHD) に加え、近年ではヘッドマウントディスプレイ (Head-Mounted Device: HMD) やジェスチャ認識を用いた手法が模索されており、それぞれの特性や課題に関する研究が行われている。

Kyriakou ら [8] は、博物館における展示物の接触不可という制約に着目し、 Natural Interaction (NI) を用いた AR システムを提案している。彼らは、多くの博物館において展示品が保護のためにガラスケース内に収められており、来館者が物理的に触れることができないという課題に対し、スマートフォンの画面を介した従来の操作ではなく、人間の自然な手の動きを利用したインタラクションの有効性を検証した。
具体的には、図 \ref{fig:kyriakou_setup} に示すように、市販のスマートフォンを HMD ケースに装着し、 Leap Motion コントローラを組み合わせることで、低コストながら素手で 3D レプリカを操作（掴むことや回転させること等）できるシステムを構築した。キプロスの博物館において 60 名の来館者を対象に実証実験を行った結果、参加者の多くは当初、タッチスクリーンやボタンといった従来の操作方法を期待していたため、システムの使用に平均して約 1 分の慣れ（トレーニング）を要した。しかし、システムの使用を自然で楽しいと感じることが示され、 NI が文化遺産へのアクセシビリティ向上に寄与すると結論付けている。

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=70mm]{./figs/関連研究/4インタラクション方式比較/kyriakou_setup.png}
\caption{スマートフォンと Leap Motion を組み合わせた NI システムの構成 [8].}
\label{fig:kyriakou_setup}
\end{center}
\end{figure}

一方で、普及している HHD と、没入感の高い HMD との間には、インタラクションパターンの設計において明確な差異が存在する。 Liu ら [9] は、博物館のガイドツアーにおける HHD と HMD (Microsoft HoloLens 2) のインタラクションパターンの比較検証を行っている。彼らは、ローマ時代の遺跡を題材とした既存の屋外用 HHD 向け AR アプリ Spirit のコンテンツを HoloLens 2 へ移植する過程を通じて、両デバイスの技術的および体験的な違いを分析した。
その結果、 HHD で一般的である画面タッチや GPS による位置情報トリガーといった手法は、 HMD におけるエアタップ (Air Tap) や SLAM (Simultaneous Localization and Mapping) といった技術と単純に置換できるものではないことが明らかになった。特に、 HMD では HHD のように画面上のテキストを読む体験が快適ではない点や、 HoloLens 2 には GPS が内蔵されていないため屋外での広域ナビゲーションが困難である点などの課題が浮き彫りとなった。これらを踏まえ、彼らはデバイスの特性に応じたインタラクションパターンの再設計が必要であると論じている。

さらに、 HMD を用いた AR アプリケーションを初めて利用する来館者が直面する課題について、 Liu ら [10] は自然史博物館におけるクジラの骨格展示を用いたプロトタイプ開発とユーザビリティ評価を行っている。彼らは、 HMD 特有の操作方法（ハンドメニューの呼び出しや視線入力など）が、未経験者にとって高いハードルとなることを指摘し、オンボーディング（導入指導）、データ収集、フィードバックの 3 段階からなる評価手法を提案した。
HoloLens 2 を用いた実証実験（図 \ref{fig:whale_interaction}）の結果、参加者はアバターへの追従や展示情報の探索といったタスクを概ね遂行できたものの、一度学習したジェスチャ操作を忘れてしまう問題や、デバイスの視野角 (Field of View: FOV) の制限によりホログラムが見切れてしまうといったハードウェア由来の課題が確認された。また、評価者が横について安全を確保しつつ観察を行う手法の有効性が示され、 HMD を用いた展示体験においては、初心者が操作学習に費やす時間を最小限に抑え、展示内容そのものに集中できるようなインタラクション設計と導入支援が不可欠であることが示唆されている。

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=70mm]{./figs/関連研究/4インタラクション方式比較/whale_interaction.png}
\caption{クジラの骨格標本を用いた HMD 実証実験の様子 [10].}
\label{fig:whale_interaction}
\end{center}
\end{figure}

\section{XR 技術を活用した博物館および史跡における展示ガイドと地域活性化}

博物館や史跡における展示体験の向上を目的として、 MR (Mixed Reality) や VR (Virtual Reality) 、 AR (Augmented Reality) といった XR 技術の導入が進められている。これらの技術は、従来の物理的な展示と人間によるガイドの制約を補完し、来館者のエンゲージメントを高める手法として注目されている。

Hammady ら [11] は、博物館における従来のガイドサービスの役割を再定義し、 MR 技術を用いたホログラフィック・バーチャルガイドシステム MuseumEye を提案している。彼らは、エジプト考古学博物館におけるツタンカーメン展示を対象に、 Microsoft HoloLens を用いたシステムを開発した。本システムは、 3D スキャンされた展示品やテキスト、画像などのマルチメディア情報を空間上に配置し、歴史的なアバター（バーチャルガイド）が来館者にストーリーテリングを行うものである。技術的な特徴として、 MuseumEye は 5 つのインタラクションレベル（物理環境、物理的展示物、バーチャルガイド、バーチャル展示物、 UI ）を提供し、図 \ref{fig:MuseumEye_UI} に示すように、ユーザーはハンドジェスチャ（エアタップ）を用いて空間上のフローティング UI を直感的に操作を行うことができる。また、展示空間にゲーム性を取り入れた Knowledge Scale game を導入することで、来館者の能動的な学習を促進している。 171 名の参加者を対象とした評価実験の結果、ガイド役 (Role of Guide) という構成概念が、システムの有用性 (Usefulness) や使いやすさ (Ease of Use) と、将来の利用意図 (Intention to Use) との関係を媒介する重要な要因であることが明らかになった。これは、 MR ガイドが単なる情報提示ツールではなく、人間のガイドと同様に Pathfinder （先導者）や Mentor （助言者）としての役割を果たすことで、来館者の体験価値を向上させることを示唆している。一方で、 HoloLens の視野角 (FOV) の狭さや、装着の不快感といったハードウェアに起因する課題も報告されている。

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=120mm]{./figs/関連研究/5デジタルガイダンス/MuseumEye_UI.png}
\caption[MuseumEyeのフローティングUIとハンドジェスチャによる操作の様子]{MuseumEyeのフローティングUIとハンドジェスチャによる操作の様子 [11].}
\label{fig:MuseumEye_UI}
\end{center}
\end{figure}

また、地域固有の文化財に対する理解促進と地域活性化（シビックプライドの醸成）を目的とした XR コンテンツの活用事例として、井上と長澤による研究 [12] が挙げられる。彼らは、神奈川県綾瀬市にある国指定史跡神崎遺跡を対象に、 VR および AR コンテンツを制作し、その効果を検証している。神崎遺跡のような埋蔵文化財は、遺構が埋め戻されているため視覚的なイメージを掴みにくいという課題があった。これに対し、井上らは発掘調査データに基づき、図 \ref{fig:Kanzaki_3DCG} のように環濠や竪穴住居を 3DCG で復元した。 VR コンテンツ VR 神崎遺跡では、 HTC Vive Pro を使用し、あえて自由移動ではなくカメラが自動で移動する方式を採用することで、 VR 酔いの防止と操作に不慣れな利用者への配慮を行っている。さらに、現地での体験を強化するために開発された AR 神崎遺跡は、 Unity と AR Foundation を用いて制作されたモバイルアプリケーションである。これは、史跡内の解説看板に設置された QR コードを読み取ることで、スマートフォンのカメラ映像に当時の集落の様子（環濠や住居）を重畳表示させるものである。これらのコンテンツを用いた実証実験の結果、参加者からは文化財のスケール感や位置関係の理解が深まったとの評価が得られ、地域住民のシビックプライド向上や観光資源としての魅力向上に寄与する可能性が示された。

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=120mm]{./figs/関連研究/5デジタルガイダンス/Kanzaki_3DCG.png}
\caption[発掘調査データに基づいて復元された神崎遺跡の3DCG]{発掘調査データに基づいて復元された神崎遺跡の3DCG [12].}
\label{fig:Kanzaki_3DCG}
\end{center}
\end{figure}

以上の先行研究から、 MR や AR を用いたガイドシステムは、展示物の視覚的な復元や情報付加により、来館者の理解と関与を深める上で有効であることが示されている。しかし、デバイスの視野角制限や操作性の学習コスト、あるいは現地への導入における運用上の課題など、解決すべき点も残されている。次節では、こうした一方的な情報提示を超え、ユーザーの行動や状態をシステムが理解し活用する双方向的なアプローチについて述べる。

\section{ユーザの状態推定と行動ログ活用に基づく双方向的な AR 鑑賞支援}

博物館や展示施設における AR (拡張現実) 技術の活用は、単にデジタルコンテンツを一方的に展示空間へ重畳表示する段階を超え、ユーザの鑑賞行動や生体反応をシステム側が取得および分析することで、より質の高い双方向的な鑑賞体験を提供するフェーズへと移行している。

Hou ら [13] は、 Microsoft HoloLens を用いた博物館向けの AR 鑑賞アプリケーションにおいて、ユーザのインタラクション情報をシステムが収集し、それを展示表現に還元する仕組みを提案している。本システムでは、展示物に対する解説テキストや 3D モデルの表示といった基本機能に加え、視線入力 (Gaze) やジェスチャ、音声コマンドといったマルチモーダルな入力をサポートしている。特筆すべきは、将来のデータ分析のためにユーザの仮想情報に対する操作を記録するタスクが設計されている点である。具体的には、図\ref{fig:hou_interface}に示すように、先行する来館者のインタラクション履歴（クリック数）をシステムが蓄積し、そのデータに基づいて展示タイトルの明るさ (Lightness) を変化させる機能が実装されている。図中のリスト項目右側に付記された数値は過去のユーザによる参照回数を示しており、システムはこの数値に基づいてユーザからどの展示項目が注目されているかという情報を可視化している。これを視覚的なポピュラリティとして後続のユーザへフィードバックすることで、静的な情報提示に留まらない動的な展示空間を構築している。

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=120mm]{./figs/関連研究/6双方向インタラクション/Info_View_and_Info_Detail_View.png}
\caption[展示情報のリスト表示と詳細表示のUI]{展示情報のリスト表示（左）と詳細表示（右）のUI [13].}
\label{fig:hou_interface}
\end{center}
\end{figure}

モバイル端末を用いたアプローチにおいて、赤嶺ら [14] は、展示者側からの情報提供のみならず、来館者の個々の展示物に対する興味度や反応を推定することを目的とした双方向メディア型ガイダンスシステムを開発した。この研究では、タブレットやスマートフォン上で動作する AR 技術を用いながら、鑑賞者の動線や興味を推定する手法を組み込んでいる。システムはユーザの滞在時間や操作履歴といった行動データを取得および解析することで、ユーザが何に関心を持っているかを判断し、その興味に関連する類似の展示物を自動的に検索かつ提示する機能を備えている。これは、 AR システムがユーザの状態を受動的に観測するだけでなく、取得した情報を能動的に活用して、個々のユーザに最適化された学習支援を行おうとする試みであると言える。

さらに近年では、高度なセンシング技術を有する HMD (ヘッドマウントディスプレイ) を活用し、より詳細なユーザ情報を取得および活用する研究が進められている。星野 [15] は、 Meta Quest 3 を用いた AR 型遠隔学習支援システム AI Aquarium において、デバイスに搭載された環境察知カメラや IMU (慣性計測装置) を用いて、ユーザの視点や頭の動き、位置情報を高精度に測定する手法を採用した。本システムの全体像を図\ref{fig:ai_aquarium_system}に示す。図中の概念図にあるように、本システムではこれらのセンシングデータに基づいて鑑賞視点の同期を図るだけでなく、閲覧ログ情報をユーザの興味関心を分析するための基礎情報として蓄積している。さらに、蓄積されたログデータと、オントロジーサーバによって構造化された知識情報を組み合わせることで、閲覧者の属性や過去の興味傾向に基づいた最適なコンテンツを提案する機能（閲覧者属性によるコンテンツ提案機能および閲覧ログによる適正コンテンツ表示機能）の実装が試みられている。これにより、システムはユーザの無意識的な鑑賞行動からも情報を取得し、対話的かつ適応的な学習環境の形成を実現している。

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=120mm]{./figs/関連研究/6双方向インタラクション/System_Concept_Diagram.png}
\caption[AI Aquariumのシステム概念図]{AI Aquariumのシステム概念図 [15].}
\label{fig:ai_aquarium_system}
\end{center}
\end{figure}

以上のように、近年の AR 鑑賞支援システムにおいては、展示情報の提示という出力の側面だけでなく、視線や動作、操作ログといったユーザ情報の入力と解析が重要な要素となっており、それらを活用した双方向的なインタラクションデザインが研究の潮流となっている。最後に、これらの高度な AR/VR 体験をモバイル環境で実現するための技術的課題である描画処理と計算オフロードに関する研究を紹介する。

\section{モバイル XR におけるリモートレンダリングと計算オフロード}

モバイル VR デバイスにおける描画性能の制約と、クロスプラットフォーム開発のコスト削減を目的として、 Seligmann [16] は、 Web ベースの VR クライアントを用いたリモートレンダリングシステムを提案している。従来の VR アプリケーションはデバイスごとに異なる API や SDK への対応が必要であり、開発コストが増大するという課題があった。これに対し、 Seligmann は Web ブラウザ上で動作するフレームワークである A-Frame と、 WebRTC によるリアルタイム通信を組み合わせることで、特定のハードウェアに依存しない汎用的なクライアントシステムを構築した（図\ref{fig:seligmann_arch}参照）。提案システムでは、 Unity で構築されたサーバー側でレンダリング処理を行い、その結果を映像ストリームとしてクライアントへ送信する。また、レイテンシの影響を軽減するために、サーバー側で生成したカスタムキューブマップを用いる手法を採用した。評価実験では Oculus Quest をクライアントとして用いたが、動画のデコード処理やシェーダーの負荷により、滑らかな再生に必要とされる 60 fps を維持することが困難であり、デコード処理の最適化が課題として残された。

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=80mm]{./figs/関連研究/7リモートレンダリング/seligmann_arch.png}
\caption[Webベースのリモートレンダリングシステムのアーキテクチャ]{Webベースのリモートレンダリングシステムのアーキテクチャ概要 [16].}
\label{fig:seligmann_arch}
\end{center}
\end{figure}

リモートレンダリングにおけるフレームタイミングの不一致とそれに伴う遅延の問題に対し、 Kelkkanen [17] は、サーバーとクライアント間で厳密な同期を取る同期型リモートレンダリング (Synchronous Remote Rendering) アーキテクチャを提案している（図\ref{fig:kelkkanen_sync}参照）。従来非同期で行われていたレンダリングと表示のプロセスを同期させることで、フレームドロップやティアリングの発生を防ぎ、ネットワーク条件が良好な場合にはローカルレンダリングと同等の低遅延 (Local Latency Mode) を実現することを目指した。彼らは NVIDIA の GPU (GTX Titan X 等) が持つハードウェアエンコーダ (NVENC) を活用し、信頼性 UDP (RUDP) を用いた独自の通信プロトコルを実装した。実験では、 LAN 環境および 50 km 離れたサーバーとのイントラネット環境、 5G ネットワーク環境下で評価を行い、有線 LAN 接続においては HTC Vive のネイティブ解像度と 90 fps のフレームレートで安定した動作を確認した。一方で、 5G や不安定なネットワーク環境下では、遅延注入 (Delay Injection) や解像度の動的変更を行うことで、フレームレートの維持を図る必要性が示された。

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=80mm]{./figs/関連研究/7リモートレンダリング/kelkkanen_sync.png}
\caption[同期型リモートレンダリングの3つの状態]{同期型リモートレンダリングシステムにおける3つの状態 [17].}
\label{fig:kelkkanen_sync}
\end{center}
\end{figure}

さらに近年では、レンダリングだけでなく、計算負荷の高い認識処理をエッジサーバーにオフロードするエッジネイティブなアプリケーションも提案されている。 Hammad ら [18] は、モバイル AR ゲームにおける身体動作認識の負荷をエッジコンピューティングで解決するシステム V-Light を開発した（図\ref{fig:vlight_arch}参照）。モバイルデバイス単体では困難な、高精度な多人数姿勢推定 (OpenPose) をエッジサーバー上で実行し、その結果を用いてリアルタイムな AR シューティングゲームを実現した。 V-Light のシステムアーキテクチャは、画像処理パイプラインとゲームプレイパイプラインを分離し、画像データは Unity から Gabriel クライアントを通じてサーバーへ送信され、処理された姿勢データのみがクライアントへ返送される仕組みとなっている。これにより、帯域幅の節約と応答性の向上を図っている。彼らはこのシステムを通じて、計算負荷の高い処理をエッジへオフロードすることで、従来のモバイル AR では実現不可能であった身体性を伴う同期型マルチプレイ体験が可能になることを示した。

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=80mm]{./figs/関連研究/7リモートレンダリング/vlight_arch.png}
\caption[V-Lightシステムアーキテクチャ]{エッジコンピューティングを活用したV-Lightのシステムアーキテクチャ [18].}
\label{fig:vlight_arch}
\end{center}
\end{figure}
